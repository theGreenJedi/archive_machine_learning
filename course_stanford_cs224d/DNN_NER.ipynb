{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os, re, json\n",
    "import getpass\n",
    "import sys\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import tensorflow as tf\n",
    "from q2_initialization import xavier_weight_init\n",
    "import utils2 as du\n",
    "import ner as ner\n",
    "from utils import data_iterator\n",
    "from model import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    embed_size = 50\n",
    "    batch_size = 64\n",
    "    label_size = 5\n",
    "    hidden_size = 100\n",
    "    max_epochs = 24 \n",
    "    early_stopping = 2\n",
    "    dropout = 0.9\n",
    "    lr = 0.001\n",
    "    l2 = 0.001\n",
    "    window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the starter word vectors\n",
    "def invert_dict(d):\n",
    "    return {v:k for k,v in d.items()}\n",
    "\n",
    "def load_wv(vocabfile, wvfile):\n",
    "    wv = loadtxt(wvfile, dtype=float)\n",
    "    with open(vocabfile) as fd:\n",
    "        words = [line.strip() for line in fd]\n",
    "    num_to_word = dict(enumerate(words))\n",
    "    #print(num_to_word)\n",
    "    word_to_num = invert_dict(num_to_word)\n",
    "    return wv, word_to_num, num_to_word\n",
    "\n",
    "wv, word_to_num, num_to_word = load_wv('data/ner/vocab.txt', 'data/ner/wordVectors.txt')\n",
    "tagnames = ['O', 'LOC', 'MISC', 'ORG', 'PER']\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = {v:k for k,v in num_to_tag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "\n",
    "def load_dataset(fname):\n",
    "    docs = []\n",
    "    with open(fname) as fd:\n",
    "        cur = []\n",
    "        for line in fd:\n",
    "            # new sentence on -DOCSTART- or blank line\n",
    "            if re.match(r\"-DOCSTART-.+\", line) or (len(line.strip()) == 0):\n",
    "                if len(cur) > 0:\n",
    "                    docs.append(cur)\n",
    "                cur = []\n",
    "            else: # read in tokens\n",
    "                cur.append(line.strip().split(\"\\t\",1))\n",
    "        # flush running buffer\n",
    "        docs.append(cur)\n",
    "    return docs\n",
    "\n",
    "def flatten1(lst):\n",
    "    return list(itertools.chain.from_iterable(lst))\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"UUUNKKK\" # unknown token\n",
    "\n",
    "# For window models\n",
    "def seq_to_windows(words, tags, word_to_num, tag_to_num, left=1, right=1):\n",
    "    ns = len(words)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(ns):\n",
    "        if words[i] == \"<s>\" or words[i] == \"</s>\":\n",
    "            continue # skip sentence delimiters\n",
    "        tagn = tag_to_num[tags[i]]\n",
    "        idxs = [word_to_num[words[ii]]\n",
    "                for ii in range((i) - int(left), (i) + int(right) + 1)]\n",
    "        X.append(idxs)\n",
    "        y.append(tagn)\n",
    "    return array(X), array(y)    \n",
    "    \n",
    "def pad_sequence(seq, left=1, right=1):\n",
    "    return (int(left)*[(\"<s>\", \"\")]) + seq + (int(right)*[(\"</s>\", \"\")])    \n",
    "    \n",
    "def docs_to_windows(docs, word_to_num, tag_to_num, wsize=3):\n",
    "    pad = (wsize - 1)/2\n",
    "    docs = flatten1([pad_sequence(seq, left=pad, right=pad) for seq in docs])\n",
    "\n",
    "    words, tags = zip(*docs)\n",
    "    words = [canonicalize_word(w, word_to_num) for w in words]\n",
    "    tags = [t.split(\"|\")[0] for t in tags]\n",
    "    return seq_to_windows(words, tags, word_to_num, tag_to_num, pad, pad)\n",
    "\n",
    "### Box Main\n",
    "docs = load_dataset('data/ner/train')\n",
    "#print(docs[0])\n",
    "X_train, y_train = docs_to_windows(docs, word_to_num, tag_to_num, wsize=config.window_size)\n",
    "if debug:\n",
    "    X_train = X_train[:1024]\n",
    "    y_train = y_train[:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = load_dataset('data/ner/dev')\n",
    "X_dev, y_dev = docs_to_windows(docs, word_to_num, tag_to_num, wsize=config.window_size)\n",
    "if debug:\n",
    "    X_dev = X_dev[:1024]\n",
    "    y_dev = y_dev[:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the test set (dummy labels only)\n",
    "docs = load_dataset('data/ner/test.masked')\n",
    "X_test, y_test = docs_to_windows(docs, word_to_num, tag_to_num, wsize=config.window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NERModel(LanguageModel):\n",
    "\n",
    "#     def load_data(self, debug=False):\n",
    "        # Load the starter word vectors\n",
    "#         self.wv, word_to_num, num_to_word = ner.load_wv('data/ner/vocab.txt', 'data/ner/wordVectors.txt')\n",
    "#         tagnames = ['O', 'LOC', 'MISC', 'ORG', 'PER']\n",
    "#         self.num_to_tag = dict(enumerate(tagnames))\n",
    "#         tag_to_num = {v:k for k,v in self.num_to_tag.iteritems()}\n",
    "\n",
    "#         # Load the training set\n",
    "#         docs = du.load_dataset('data/ner/train')\n",
    "#         self.X_train, self.y_train = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n",
    "#         if debug:\n",
    "#             self.X_train = self.X_train[:1024]\n",
    "#             self.y_train = self.y_train[:1024]\n",
    "\n",
    "#         # Load the dev set (for tuning hyperparameters)\n",
    "#         docs = du.load_dataset('data/ner/dev')\n",
    "#         self.X_dev, self.y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n",
    "#         if debug:\n",
    "#             self.X_dev = self.X_dev[:1024]\n",
    "#             self.y_dev = self.y_dev[:1024]\n",
    "\n",
    "#         # Load the test set (dummy labels only)\n",
    "#         docs = du.load_dataset('data/ner/test.masked')\n",
    "#         self.X_test, self.y_test = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n",
    "\n",
    "    def add_placeholders(self):\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(tf.int32, shape=[None, self.config.window_size], name='Input')\n",
    "        self.labels_placeholder = tf.placeholder(tf.float32, shape=[None, self.config.label_size], name='Target')\n",
    "        self.dropout_placeholder = tf.placeholder(tf.float32, name='Dropout')\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, input_batch, dropout, label_batch=None):\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {self.input_placeholder: input_batch, }\n",
    "        if label_batch is not None:\n",
    "            feed_dict[self.labels_placeholder] = label_batch\n",
    "        if dropout is not None:\n",
    "            feed_dict[self.dropout_placeholder] = dropout\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_embedding(self):\n",
    "\n",
    "        # The embedding lookup is currently only implemented for the CPU\n",
    "        with tf.device('/cpu:0'):\n",
    "            ### YOUR CODE HERE\n",
    "            embedding = tf.get_variable('Embedding', [len(wv), self.config.embed_size])\n",
    "            window = tf.nn.embedding_lookup(embedding, self.input_placeholder)\n",
    "            window = tf.reshape(window, [-1, self.config.window_size * self.config.embed_size])\n",
    "            ### END YOUR CODE\n",
    "            return window\n",
    "\n",
    "    def add_model(self, window):\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        with tf.variable_scope('Layer1', initializer=xavier_weight_init()) as scope:\n",
    "            W = tf.get_variable('W', [self.config.window_size * self.config.embed_size, self.config.hidden_size])\n",
    "            b1 = tf.get_variable('b1', [self.config.hidden_size])\n",
    "            h = tf.nn.tanh(tf.matmul(window, W) + b1)\n",
    "            if self.config.l2:\n",
    "                tf.add_to_collection('total_loss', 0.5 * self.config.l2 * tf.nn.l2_loss(W))\n",
    "\n",
    "        with tf.variable_scope('Layer2', initializer=xavier_weight_init()) as scope:\n",
    "            U = tf.get_variable('U', [self.config.hidden_size, self.config.label_size])\n",
    "            b2 = tf.get_variable('b2', [self.config.label_size])\n",
    "            y = tf.matmul(h, U) + b2\n",
    "            if self.config.l2:\n",
    "                tf.add_to_collection('total_loss', 0.5 * self.config.l2 * tf.nn.l2_loss(U))\n",
    "        output = tf.nn.dropout(y, self.dropout_placeholder)\n",
    "        ### END YOUR CODE\n",
    "        return output \n",
    "\n",
    "    def add_loss_op(self, y):\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, self.labels_placeholder))\n",
    "        tf.add_to_collection('total_loss', cross_entropy)\n",
    "        loss = tf.add_n(tf.get_collection('total_loss'))\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        optimizer = tf.train.AdamOptimizer(self.config.lr)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        #self.load_data(debug=False)\n",
    "        self.add_placeholders()\n",
    "        window = self.add_embedding()\n",
    "        y = self.add_model(window)\n",
    "\n",
    "        self.loss = self.add_loss_op(y)\n",
    "        self.predictions = tf.nn.softmax(y)\n",
    "        one_hot_prediction = tf.argmax(self.predictions, 1)\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.labels_placeholder, 1), one_hot_prediction)\n",
    "        self.correct_predictions = tf.reduce_sum(tf.cast(correct_prediction, 'int32'))\n",
    "        self.train_op = self.add_training_op(self.loss)\n",
    "\n",
    "    def run_epoch(self, session, input_data, input_labels, shuffle=True, verbose=True):\n",
    "        orig_X, orig_y = input_data, input_labels\n",
    "        dp = self.config.dropout\n",
    "        # We're interested in keeping track of the loss and accuracy during training\n",
    "        total_loss = []\n",
    "        total_correct_examples = 0\n",
    "        total_processed_examples = 0\n",
    "        total_steps = len(orig_X) / self.config.batch_size\n",
    "        for step, (x, y) in enumerate(data_iterator(orig_X, orig_y, batch_size=self.config.batch_size,label_size=self.config.label_size, shuffle=shuffle)):\n",
    "            feed = self.create_feed_dict(input_batch=x, dropout=dp, label_batch=y)\n",
    "            loss, total_correct, _ = session.run([self.loss, self.correct_predictions, self.train_op],feed_dict=feed)\n",
    "            total_processed_examples += len(x)\n",
    "            total_correct_examples += total_correct\n",
    "            total_loss.append(loss)\n",
    "            ##\n",
    "            if verbose and step % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(\n",
    "                    step, total_steps, np.mean(total_loss)))\n",
    "                sys.stdout.flush()\n",
    "        if verbose:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.flush()\n",
    "        return np.mean(total_loss), total_correct_examples / float(total_processed_examples)\n",
    "\n",
    "    def predict(self, session, X, y=None):\n",
    "        # If y is given, the loss is also calculated\n",
    "        # We deactivate dropout by setting it to 1\n",
    "        dp = 1\n",
    "        losses = []\n",
    "        results = []\n",
    "        if np.any(y):\n",
    "            data = data_iterator(X, y, batch_size=self.config.batch_size,label_size=self.config.label_size, shuffle=False)\n",
    "        else:\n",
    "            data = data_iterator(X, batch_size=self.config.batch_size,label_size=self.config.label_size, shuffle=False)\n",
    "        for step, (x, y) in enumerate(data):\n",
    "            feed = self.create_feed_dict(input_batch=x, dropout=dp)\n",
    "            if np.any(y):\n",
    "                feed[self.labels_placeholder] = y\n",
    "                loss, preds = session.run([self.loss, self.predictions], feed_dict=feed)\n",
    "                losses.append(loss)\n",
    "            else:\n",
    "                preds = session.run(self.predictions, feed_dict=feed)\n",
    "            predicted_indices = preds.argmax(axis=1)\n",
    "            results.extend(predicted_indices)\n",
    "        return np.mean(losses), results\n",
    "\n",
    "def print_confusion(confusion, num_to_tag):\n",
    "    # Summing top to bottom gets the total number of tags guessed as T\n",
    "    total_guessed_tags = confusion.sum(axis=0)\n",
    "    # Summing left to right gets the total number of true tags\n",
    "    total_true_tags = confusion.sum(axis=1)\n",
    "    #print\n",
    "    print(confusion)\n",
    "    for i, tag in sorted(num_to_tag.items()):\n",
    "        prec = confusion[i, i] / float(total_guessed_tags[i])\n",
    "        recall = confusion[i, i] / float(total_true_tags[i])\n",
    "        print('Tag: {} - P {:2.4f} / R {:2.4f}'.format(tag, prec, recall))\n",
    "\n",
    "def calculate_confusion(config, predicted_indices, y_indices):\n",
    "    confusion = np.zeros((config.label_size, config.label_size), dtype=np.int32)\n",
    "    for i in range(len(y_indices)):\n",
    "        correct_label = y_indices[i]\n",
    "        guessed_label = predicted_indices[i]\n",
    "        confusion[correct_label, guessed_label] += 1\n",
    "    return confusion\n",
    "\n",
    "def save_predictions(predictions, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for prediction in predictions:\n",
    "            f.write(str(prediction) + \"\\n\")\n",
    "\n",
    "def test_NER():\n",
    "\n",
    "#     config = Config()\n",
    "    with tf.Graph().as_default():\n",
    "        model = NERModel(config)\n",
    "\n",
    "        init = tf.initialize_all_variables()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            best_val_loss = float('inf')\n",
    "            best_val_epoch = 0\n",
    "\n",
    "            session.run(init)\n",
    "            for epoch in range(config.max_epochs):\n",
    "                print('Epoch {}'.format(epoch))\n",
    "                start = time.time()\n",
    "                ###\n",
    "                train_loss, train_acc = model.run_epoch(session, X_train, y_train)\n",
    "                val_loss, predictions = model.predict(session, X_dev, y_dev)\n",
    "                print('Training loss: {}'.format(train_loss))\n",
    "                print('Training acc: {}'.format(train_acc))\n",
    "                print('Validation loss: {}'.format(val_loss))\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_epoch = epoch\n",
    "                    if not os.path.exists(\"./weights\"):\n",
    "                        os.makedirs(\"./weights\")\n",
    "\n",
    "                    saver.save(session, './weights/ner.weights')\n",
    "                if epoch - best_val_epoch > config.early_stopping:\n",
    "                      break\n",
    "                ###\n",
    "                confusion = calculate_confusion(config, predictions, y_dev)\n",
    "                print_confusion(confusion, num_to_tag)\n",
    "                print('Total time: {}'.format(time.time() - start))\n",
    "\n",
    "            saver.restore(session, './weights/ner.weights')\n",
    "            print('Test')\n",
    "            print('=-=-=')\n",
    "            print('Writing predictions to q2_test.predicted')\n",
    "            _, predictions = model.predict(session, X_test, y_test)\n",
    "            save_predictions(predictions, \"q2_test.predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-f1ae41e87bf0>:188 in test_NER.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 0\n",
      "3181 / 3181.578125 : loss = 0.30291557312011723Training loss: 0.3029155731201172\n",
      "Training acc: 0.9214079097931942\n",
      "Validation loss: 0.1950983703136444\n",
      "[[42340    90    92   142    95]\n",
      " [  193  1767    10    76    48]\n",
      " [  270    38   925    18    17]\n",
      " [  580   160    35  1211   106]\n",
      " [  453    43     5    36  2612]]\n",
      "Tag: O - P 0.9659 / R 0.9902\n",
      "Tag: LOC - P 0.8422 / R 0.8438\n",
      "Tag: MISC - P 0.8669 / R 0.7295\n",
      "Tag: ORG - P 0.8166 / R 0.5789\n",
      "Tag: PER - P 0.9076 / R 0.8295\n",
      "Total time: 254.55922269821167\n",
      "Epoch 1\n",
      "3181 / 3181.578125 : loss = 0.16783928871154785Training loss: 0.16783928871154785\n",
      "Training acc: 0.9637070832576208\n",
      "Validation loss: 0.1892177313566208\n",
      "[[42408    59    52   154    86]\n",
      " [  179  1767    16    91    41]\n",
      " [  313    32   868    36    19]\n",
      " [  565    92    29  1314    92]\n",
      " [  448    42     2    37  2620]]\n",
      "Tag: O - P 0.9657 / R 0.9918\n",
      "Tag: LOC - P 0.8870 / R 0.8438\n",
      "Tag: MISC - P 0.8976 / R 0.6845\n",
      "Tag: ORG - P 0.8051 / R 0.6281\n",
      "Tag: PER - P 0.9167 / R 0.8320\n",
      "Total time: 265.5355746746063\n",
      "Epoch 2\n",
      "3181 / 3181.578125 : loss = 0.14418426156044006Training loss: 0.14418426156044006\n",
      "Training acc: 0.9689030109860968\n",
      "Validation loss: 0.18940864503383636\n",
      "[[42228    72    80   313    66]\n",
      " [  160  1818     9    70    37]\n",
      " [  273    40   895    43    17]\n",
      " [  479   114    34  1398    67]\n",
      " [  517    43     4    64  2521]]\n",
      "Tag: O - P 0.9673 / R 0.9876\n",
      "Tag: LOC - P 0.8711 / R 0.8682\n",
      "Tag: MISC - P 0.8757 / R 0.7058\n",
      "Tag: ORG - P 0.7405 / R 0.6683\n",
      "Tag: PER - P 0.9309 / R 0.8006\n",
      "Total time: 259.69626903533936\n",
      "Epoch 3\n",
      "3181 / 3181.578125 : loss = 0.13252803683280945Training loss: 0.13252803683280945\n",
      "Training acc: 0.9710098663693824\n",
      "Validation loss: 0.18680280447006226\n",
      "[[42295    61    67   232   104]\n",
      " [  159  1765    13   113    44]\n",
      " [  273    30   911    31    23]\n",
      " [  490    85    31  1387    99]\n",
      " [  425    30     3    55  2636]]\n",
      "Tag: O - P 0.9691 / R 0.9891\n",
      "Tag: LOC - P 0.8955 / R 0.8429\n",
      "Tag: MISC - P 0.8888 / R 0.7185\n",
      "Tag: ORG - P 0.7629 / R 0.6630\n",
      "Tag: PER - P 0.9071 / R 0.8371\n",
      "Total time: 254.37302613258362\n",
      "Epoch 4\n",
      "3181 / 3181.578125 : loss = 0.12469461560249329Training loss: 0.12469461560249329\n",
      "Training acc: 0.9723702368616204\n",
      "Validation loss: 0.193494513630867\n",
      "[[42344    59    75   223    58]\n",
      " [  147  1826    11    71    39]\n",
      " [  272    33   900    46    17]\n",
      " [  543   113    28  1336    72]\n",
      " [  555    38     4    57  2495]]\n",
      "Tag: O - P 0.9654 / R 0.9903\n",
      "Tag: LOC - P 0.8826 / R 0.8720\n",
      "Tag: MISC - P 0.8841 / R 0.7098\n",
      "Tag: ORG - P 0.7709 / R 0.6386\n",
      "Tag: PER - P 0.9306 / R 0.7923\n",
      "Total time: 270.1177611351013\n",
      "Epoch 5\n",
      "3181 / 3181.578125 : loss = 0.11936861276626587Training loss: 0.11936861276626587\n",
      "Training acc: 0.9736127413184298\n",
      "Validation loss: 0.192837193608284\n",
      "[[42420    36    69   168    66]\n",
      " [  202  1729    26    91    46]\n",
      " [  294    18   918    23    15]\n",
      " [  561    88    36  1339    68]\n",
      " [  548    20     6    47  2528]]\n",
      "Tag: O - P 0.9635 / R 0.9921\n",
      "Tag: LOC - P 0.9143 / R 0.8257\n",
      "Tag: MISC - P 0.8701 / R 0.7240\n",
      "Tag: ORG - P 0.8028 / R 0.6401\n",
      "Tag: PER - P 0.9284 / R 0.8028\n",
      "Total time: 261.07048654556274\n",
      "Epoch 6\n",
      "3181 / 3181.578125 : loss = 0.11575712263584137Training loss: 0.11575712263584137\n",
      "Training acc: 0.9742168047500013\n",
      "Validation loss: 0.19146275520324707\n",
      "Test\n",
      "=-=-=\n",
      "Writing predictions to q2_test.predicted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "test_NER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
