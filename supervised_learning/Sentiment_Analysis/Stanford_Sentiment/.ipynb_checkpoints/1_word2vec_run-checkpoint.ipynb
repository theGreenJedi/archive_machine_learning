{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10010: 14.000660\n",
      "iter 10020: 13.886994\n",
      "iter 10030: 13.817060\n",
      "iter 10040: 13.773334\n",
      "iter 10050: 13.764332\n",
      "iter 10060: 13.662535\n",
      "iter 10070: 13.508183\n",
      "iter 10080: 13.508331\n",
      "iter 10090: 13.548322\n",
      "iter 10100: 13.523937\n",
      "iter 10110: 13.599342\n",
      "iter 10120: 13.598409\n",
      "iter 10130: 13.512046\n",
      "iter 10140: 13.544682\n",
      "iter 10150: 13.447477\n",
      "iter 10160: 13.421038\n",
      "iter 10170: 13.471382\n",
      "iter 10180: 13.406297\n",
      "iter 10190: 13.331623\n",
      "iter 10200: 13.311980\n",
      "iter 10210: 13.187346\n",
      "iter 10220: 13.129141\n",
      "iter 10230: 13.111728\n",
      "iter 10240: 13.220341\n",
      "iter 10250: 13.111148\n",
      "iter 10260: 13.111354\n",
      "iter 10270: 13.058373\n",
      "iter 10280: 13.094438\n",
      "iter 10290: 13.064875\n",
      "iter 10300: 13.027899\n"
     ]
    }
   ],
   "source": [
    "# Acknowledgement\n",
    "# CS224d: Deep Learning for Natural Language Processing\n",
    "# Dataset: http://nlp.stanford.edu/sentiment/index.html\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\deep_learning_archieves\\\\modules')\n",
    "#sys.path.insert(0, 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\deep_learning_archieves\\\\datasets')\n",
    "from cs224d.data_utils import *\n",
    "from word2vec import *\n",
    "from sgd import *\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / \\\n",
    "\tdimVectors, np.zeros((nWords, dimVectors))), axis=0)\n",
    "wordVectors0 = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, \n",
    "    \tnegSamplingCostAndGradient), \n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "print \"sanity check: cost at convergence should be around or below 10\"\n",
    "\n",
    "# sum the input and output word vectors\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "\n",
    "# Visualize the word vectors you trained\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \n",
    "\t\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "\t\"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \n",
    "\t\"annoying\"]\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2]) \n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i], \n",
    "    \tbbox=dict(facecolor='green', alpha=0.1))\n",
    "    \n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
