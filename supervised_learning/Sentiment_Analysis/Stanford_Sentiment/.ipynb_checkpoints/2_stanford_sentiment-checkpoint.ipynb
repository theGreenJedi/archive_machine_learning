{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for reg=0.000000\n",
      "iter 100: 1.570773\n",
      "iter 200: 1.570756\n",
      "iter 300: 1.570724\n",
      "iter 400: 1.570679\n",
      "iter 500: 1.570622\n",
      "iter 600: 1.570554\n",
      "iter 700: 1.570477\n",
      "iter 800: 1.570391\n",
      "iter 900: 1.570297\n",
      "iter 1000: 1.570197\n",
      "iter 1100: 1.570091\n",
      "iter 1200: 1.569979\n",
      "iter 1300: 1.569863\n",
      "iter 1400: 1.569744\n",
      "iter 1500: 1.569621\n",
      "iter 1600: 1.569495\n",
      "iter 1700: 1.569366\n",
      "iter 1800: 1.569236\n",
      "iter 1900: 1.569104\n",
      "iter 2000: 1.568971\n",
      "iter 2100: 1.568837\n",
      "iter 2200: 1.568702\n",
      "iter 2300: 1.568567\n",
      "iter 2400: 1.568432\n",
      "iter 2500: 1.568296\n",
      "iter 2600: 1.568161\n",
      "iter 2700: 1.568026\n",
      "iter 2800: 1.567892\n",
      "iter 2900: 1.567758\n",
      "iter 3000: 1.567625\n",
      "iter 3100: 1.567493\n",
      "iter 3200: 1.567361\n",
      "iter 3300: 1.567231\n",
      "iter 3400: 1.567102\n",
      "iter 3500: 1.566974\n",
      "iter 3600: 1.566847\n",
      "iter 3700: 1.566722\n",
      "iter 3800: 1.566598\n",
      "iter 3900: 1.566475\n",
      "iter 4000: 1.566354\n",
      "iter 4100: 1.566233\n",
      "iter 4200: 1.566115\n",
      "iter 4300: 1.565998\n",
      "iter 4400: 1.565882\n",
      "iter 4500: 1.565768\n",
      "iter 4600: 1.565655\n",
      "iter 4700: 1.565544\n",
      "iter 4800: 1.565434\n",
      "iter 4900: 1.565326\n",
      "iter 5000: 1.565219\n",
      "iter 5100: 1.565113\n",
      "iter 5200: 1.565009\n",
      "iter 5300: 1.564907\n",
      "iter 5400: 1.564806\n",
      "iter 5500: 1.564706\n",
      "iter 5600: 1.564608\n",
      "iter 5700: 1.564511\n",
      "iter 5800: 1.564416\n",
      "iter 5900: 1.564322\n",
      "iter 6000: 1.564229\n",
      "iter 6100: 1.564137\n",
      "iter 6200: 1.564047\n",
      "iter 6300: 1.563959\n",
      "iter 6400: 1.563871\n",
      "iter 6500: 1.563785\n",
      "iter 6600: 1.563700\n",
      "iter 6700: 1.563617\n",
      "iter 6800: 1.563534\n",
      "iter 6900: 1.563453\n",
      "iter 7000: 1.563373\n",
      "iter 7100: 1.563294\n",
      "iter 7200: 1.563217\n",
      "iter 7300: 1.563140\n",
      "iter 7400: 1.563065\n",
      "iter 7500: 1.562991\n",
      "iter 7600: 1.562918\n",
      "iter 7700: 1.562845\n",
      "iter 7800: 1.562774\n",
      "iter 7900: 1.562704\n",
      "iter 8000: 1.562635\n",
      "iter 8100: 1.562568\n",
      "iter 8200: 1.562501\n",
      "iter 8300: 1.562435\n",
      "iter 8400: 1.562370\n",
      "iter 8500: 1.562306\n",
      "iter 8600: 1.562242\n",
      "iter 8700: 1.562180\n",
      "iter 8800: 1.562119\n",
      "iter 8900: 1.562058\n",
      "iter 9000: 1.561999\n",
      "iter 9100: 1.561940\n",
      "iter 9200: 1.561882\n",
      "iter 9300: 1.561825\n",
      "iter 9400: 1.561769\n",
      "iter 9500: 1.561714\n",
      "iter 9600: 1.561659\n",
      "iter 9700: 1.561605\n",
      "iter 9800: 1.561552\n",
      "iter 9900: 1.561500\n",
      "iter 10000: 1.561448\n",
      "Train accuracy (%): 29.529494\n",
      "Dev accuracy (%): 30.517711\n",
      "Training for reg=0.000010\n",
      "iter 100: 1.570990\n",
      "iter 200: 1.570973\n",
      "iter 300: 1.570941\n",
      "iter 400: 1.570896\n",
      "iter 500: 1.570839\n",
      "iter 600: 1.570771\n",
      "iter 700: 1.570693\n",
      "iter 800: 1.570608\n",
      "iter 900: 1.570515\n",
      "iter 1000: 1.570416\n",
      "iter 1100: 1.570311\n",
      "iter 1200: 1.570202\n",
      "iter 1300: 1.570088\n",
      "iter 1400: 1.569971\n",
      "iter 1500: 1.569851\n",
      "iter 1600: 1.569729\n",
      "iter 1700: 1.569605\n",
      "iter 1800: 1.569479\n",
      "iter 1900: 1.569352\n",
      "iter 2000: 1.569225\n",
      "iter 2100: 1.569097\n",
      "iter 2200: 1.568968\n",
      "iter 2300: 1.568840\n",
      "iter 2400: 1.568712\n",
      "iter 2500: 1.568584\n",
      "iter 2600: 1.568457\n",
      "iter 2700: 1.568331\n",
      "iter 2800: 1.568205\n",
      "iter 2900: 1.568081\n",
      "iter 3000: 1.567957\n",
      "iter 3100: 1.567835\n",
      "iter 3200: 1.567714\n",
      "iter 3300: 1.567595\n",
      "iter 3400: 1.567477\n",
      "iter 3500: 1.567361\n",
      "iter 3600: 1.567246\n",
      "iter 3700: 1.567132\n",
      "iter 3800: 1.567020\n",
      "iter 3900: 1.566910\n",
      "iter 4000: 1.566802\n",
      "iter 4100: 1.566695\n",
      "iter 4200: 1.566590\n",
      "iter 4300: 1.566487\n",
      "iter 4400: 1.566385\n",
      "iter 4500: 1.566285\n",
      "iter 4600: 1.566187\n",
      "iter 4700: 1.566090\n",
      "iter 4800: 1.565995\n",
      "iter 4900: 1.565902\n",
      "iter 5000: 1.565810\n",
      "iter 5100: 1.565720\n",
      "iter 5200: 1.565632\n",
      "iter 5300: 1.565545\n",
      "iter 5400: 1.565460\n",
      "iter 5500: 1.565376\n",
      "iter 5600: 1.565294\n",
      "iter 5700: 1.565214\n",
      "iter 5800: 1.565135\n",
      "iter 5900: 1.565057\n",
      "iter 6000: 1.564981\n",
      "iter 6100: 1.564907\n",
      "iter 6200: 1.564833\n",
      "iter 6300: 1.564762\n",
      "iter 6400: 1.564691\n",
      "iter 6500: 1.564622\n",
      "iter 6600: 1.564555\n",
      "iter 6700: 1.564489\n",
      "iter 6800: 1.564423\n",
      "iter 6900: 1.564360\n",
      "iter 7000: 1.564297\n",
      "iter 7100: 1.564236\n",
      "iter 7200: 1.564176\n",
      "iter 7300: 1.564117\n",
      "iter 7400: 1.564059\n",
      "iter 7500: 1.564003\n",
      "iter 7600: 1.563947\n",
      "iter 7700: 1.563893\n",
      "iter 7800: 1.563840\n",
      "iter 7900: 1.563787\n",
      "iter 8000: 1.563736\n",
      "iter 8100: 1.563686\n",
      "iter 8200: 1.563637\n",
      "iter 8300: 1.563588\n",
      "iter 8400: 1.563541\n",
      "iter 8500: 1.563495\n",
      "iter 8600: 1.563449\n",
      "iter 8700: 1.563405\n",
      "iter 8800: 1.563361\n",
      "iter 8900: 1.563318\n",
      "iter 9000: 1.563276\n",
      "iter 9100: 1.563235\n",
      "iter 9200: 1.563195\n",
      "iter 9300: 1.563155\n",
      "iter 9400: 1.563117\n",
      "iter 9500: 1.563079\n",
      "iter 9600: 1.563042\n",
      "iter 9700: 1.563005\n",
      "iter 9800: 1.562969\n",
      "iter 9900: 1.562934\n",
      "iter 10000: 1.562900\n",
      "Train accuracy (%): 29.541199\n",
      "Dev accuracy (%): 30.154405\n",
      "Training for reg=0.000030\n",
      "iter 100: 1.571418\n",
      "iter 200: 1.571400\n",
      "iter 300: 1.571367\n",
      "iter 400: 1.571320\n",
      "iter 500: 1.571261\n",
      "iter 600: 1.571191\n",
      "iter 700: 1.571113\n",
      "iter 800: 1.571026\n",
      "iter 900: 1.570932\n",
      "iter 1000: 1.570833\n",
      "iter 1100: 1.570729\n",
      "iter 1200: 1.570620\n",
      "iter 1300: 1.570508\n",
      "iter 1400: 1.570394\n",
      "iter 1500: 1.570277\n",
      "iter 1600: 1.570158\n",
      "iter 1700: 1.570038\n",
      "iter 1800: 1.569918\n",
      "iter 1900: 1.569797\n",
      "iter 2000: 1.569676\n",
      "iter 2100: 1.569555\n",
      "iter 2200: 1.569435\n",
      "iter 2300: 1.569315\n",
      "iter 2400: 1.569197\n",
      "iter 2500: 1.569079\n",
      "iter 2600: 1.568963\n",
      "iter 2700: 1.568848\n",
      "iter 2800: 1.568734\n",
      "iter 2900: 1.568622\n",
      "iter 3000: 1.568512\n",
      "iter 3100: 1.568404\n",
      "iter 3200: 1.568297\n",
      "iter 3300: 1.568192\n",
      "iter 3400: 1.568089\n",
      "iter 3500: 1.567988\n",
      "iter 3600: 1.567890\n",
      "iter 3700: 1.567793\n"
     ]
    }
   ],
   "source": [
    "# Acknowledgement\n",
    "# CS224d: Deep Learning for Natural Language Processing\n",
    "# Dataset: http://nlp.stanford.edu/sentiment/index.html\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\deep_learning_archieves\\\\modules')\n",
    "#sys.path.insert(0, 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\deep_learning_archieves\\\\datasets')\n",
    "\n",
    "from cs224d.data_utils import *\n",
    "from sgd import load_saved_params, sgd\n",
    "from softmaxreg import softmaxRegression, getSentenceFeature, accuracy, softmax_wrapper\n",
    "\n",
    "# Try different regularizations and pick the best!\n",
    "# NOTE: fill in one more \"your code here\" below before running!\n",
    "REGULARIZATION = None   # Assign a list of floats in the block below\n",
    "### YOUR CODE HERE\n",
    "REGULARIZATION = [0.0, 0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01]\n",
    "### END YOUR CODE\n",
    "\n",
    "# Load the dataset\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# Load the word vectors we trained earlier \n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "dimVectors = wordVectors.shape[1]\n",
    "\n",
    "# Load the train set\n",
    "trainset = dataset.getTrainSentences()\n",
    "nTrain = len(trainset)\n",
    "trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "for i in xrange(nTrain):\n",
    "    words, trainLabels[i] = trainset[i]\n",
    "    trainFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "# Prepare dev set features\n",
    "devset = dataset.getDevSentences()\n",
    "nDev = len(devset)\n",
    "devFeatures = np.zeros((nDev, dimVectors))\n",
    "devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "for i in xrange(nDev):\n",
    "    words, devLabels[i] = devset[i]\n",
    "    devFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "# Try our regularization parameters\n",
    "results = []\n",
    "for regularization in REGULARIZATION:\n",
    "    random.seed(3141)\n",
    "    np.random.seed(59265)\n",
    "    weights = np.random.randn(dimVectors, 5)\n",
    "    print \"Training for reg=%f\" % regularization \n",
    "\n",
    "    # We will do batch optimization\n",
    "    weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, \n",
    "        weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n",
    "\n",
    "    # Test on train set\n",
    "    _, _, pred = softmaxRegression(trainFeatures, trainLabels, weights)\n",
    "    trainAccuracy = accuracy(trainLabels, pred)\n",
    "    print \"Train accuracy (%%): %f\" % trainAccuracy\n",
    "\n",
    "    # Test on dev set\n",
    "    _, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "    devAccuracy = accuracy(devLabels, pred)\n",
    "    print \"Dev accuracy (%%): %f\" % devAccuracy\n",
    "\n",
    "    # Save the results and weights\n",
    "    results.append({\n",
    "        \"reg\" : regularization, \n",
    "        \"weights\" : weights, \n",
    "        \"train\" : trainAccuracy, \n",
    "        \"dev\" : devAccuracy})\n",
    "\n",
    "# Print the accuracies\n",
    "print \"\"\n",
    "print \"=== Recap ===\"\n",
    "print \"Reg\\t\\tTrain\\t\\tDev\"\n",
    "for result in results:\n",
    "    print \"%E\\t%f\\t%f\" % (\n",
    "        result[\"reg\"], \n",
    "        result[\"train\"], \n",
    "        result[\"dev\"])\n",
    "print \"\"\n",
    "\n",
    "# Pick the best regularization parameters\n",
    "BEST_REGULARIZATION = None\n",
    "BEST_WEIGHTS = None\n",
    "\n",
    "### YOUR CODE HERE \n",
    "best_dev = 0\n",
    "for result in results:\n",
    "    if result[\"dev\"] > best_dev:\n",
    "        best_dev = result[\"dev\"]\n",
    "        BEST_REGULARIZATION = result[\"reg\"]\n",
    "        BEST_WEIGHTS = result[\"weights\"]\n",
    "### END YOUR CODE\n",
    "\n",
    "# Test your findings on the test set\n",
    "testset = dataset.getTestSentences()\n",
    "nTest = len(testset)\n",
    "testFeatures = np.zeros((nTest, dimVectors))\n",
    "testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "for i in xrange(nTest):\n",
    "    words, testLabels[i] = testset[i]\n",
    "    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, BEST_WEIGHTS)\n",
    "print \"Best regularization value: %E\" % BEST_REGULARIZATION\n",
    "print \"Test accuracy (%%): %f\" % accuracy(testLabels, pred)\n",
    "\n",
    "# Make a plot of regularization vs accuracy\n",
    "plt.plot(REGULARIZATION, [x[\"train\"] for x in results])\n",
    "plt.plot(REGULARIZATION, [x[\"dev\"] for x in results])\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"regularization\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.savefig(\"q4_reg_v_acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
