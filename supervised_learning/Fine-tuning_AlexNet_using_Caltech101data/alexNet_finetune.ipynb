{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning AlexNet using Caltech101dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EE488C Special Topics in EE <Deep Learning and AlphaGo>, Fall 2016\n",
    "# Information Theory & Machine Learning Lab (http://itml.kaist.ac.kr), School of EE, KAIST\n",
    "# written by Jongmin Yoon\n",
    "# 2016/11/08\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "from crop_batch import crop_batch\n",
    "\n",
    "\n",
    "train_filename = \"./Caltech101_ten_train.h5\"\n",
    "val_filename = \"./Caltech101_ten_val.h5\"\n",
    "output_weight_filename = './Caltech101_finetune_weight.npy'\n",
    "# Variable setting\n",
    "NUM_BATCHES = 32 # 32\n",
    "NUM_EPOCH = 3 # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load HDF5 dataset and import as numpy array\n",
    "h5f = h5py.File(train_filename, 'r+')\n",
    "X_arr, Y_arr = h5f['/X'], h5f['/Y']\n",
    "X, Y = np.array(X_arr), np.array(Y_arr)\n",
    "h5f = h5py.File(val_filename, 'r+')\n",
    "X_arr_val, Y_arr_val = h5f['/X'], h5f['/Y']\n",
    "X_val, Y_val = np.array(X_arr_val), np.array(Y_arr_val)\n",
    "\n",
    "# Import mean of ilsvrc2012 dataset\n",
    "mean_filename = './ilsvrc_2012_mean.npy'\n",
    "mean_data = np.load(mean_filename).mean(1).mean(1)\n",
    "mean_data = mean_data.reshape((1, 1, 1, 3))\n",
    "mean_val = np.repeat(mean_data, X_val.shape[0], axis=0)\n",
    "mean_val = np.repeat(mean_val, 227, axis=1)\n",
    "mean_val = np.repeat(mean_val, 227, axis=2)\n",
    "mean_data = np.repeat(mean_data, NUM_BATCHES, axis=0)\n",
    "mean_data = np.repeat(mean_data, 256, axis=1)\n",
    "mean_data = np.repeat(mean_data, 256, axis=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make samples from validation set by making crops\n",
    "def crop_val():\n",
    "    global X_val, Y_val\n",
    "    choose_crop = random.randint(low=0, high=30, size=(X_val.shape[0], 2))\n",
    "    X_crop_val = np.zeros((X_val.shape[0], 227, 227, 3))\n",
    "\n",
    "    for i in range(X_val.shape[0]):\n",
    "        X_crop_val[i] = X_val[i, choose_crop[i, 0]:choose_crop[i, 0] + 227,\n",
    "                              choose_crop[i, 1]:choose_crop[i, 1] + 227, :]\n",
    "\n",
    "    X_crop_val = X_crop_val[:, :, :, [2, 1, 0]]\n",
    "    X_crop_val *= 255.\n",
    "    X_crop_val -= mean_val\n",
    "\n",
    "    return X_crop_val, Y_val\n",
    "\n",
    "\n",
    "Xb_val, Y_val = crop_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct LEGO blocks of AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the weight set pre-trained by AlexNet\n",
    "net_data = np.load(\"./bvlc_alexnet.npy\").item()\n",
    "for x in net_data:\n",
    "    exec (\"%s = %s\" % (str(x) + \"W\", \"tf.Variable(net_data[x][0])\"))\n",
    "    exec (\"%s = %s\" % (str(x) + \"b\", \"tf.Variable(net_data[x][1])\"))\n",
    "\n",
    "\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w, padding=\"VALID\", group=1):\n",
    "    input_groups, kernel_groups = tf.split(3, group, input), tf.split(3, group, kernel)\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    output_groups = [ convolve(i, k) for i, k in zip(input_groups, kernel_groups) ]\n",
    "    conv = tf.concat(3, output_groups)\n",
    "    return tf.reshape(tf.nn.bias_add(conv, biases), [-1] + conv.get_shape().as_list()[1:])\n",
    "\n",
    "\n",
    "# Input Layer\n",
    "x = tf.placeholder(tf.float32, shape=(None, 227, 227, 3))\n",
    "\n",
    "# Convolutional Layer 1\n",
    "conv1 = tf.nn.relu( conv(x, conv1W, conv1b, 11, 11, 96, 4, 4, padding=\"VALID\", group=1) )\n",
    "lrn1 = tf.nn.local_response_normalization(conv1, depth_radius=2, alpha=2e-5, beta=0.75, bias=1.0)\n",
    "maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Convolutional Layer 2\n",
    "conv2 = tf.nn.relu(conv(maxpool1, conv2W, conv2b, 5, 5, 256, 1, 1, padding=\"SAME\", group=2))\n",
    "lrn2 = tf.nn.local_response_normalization(conv2, depth_radius=2, alpha=2e-5, beta=0.75, bias=1.0)\n",
    "maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Convolutional Layer 3\n",
    "conv3 = tf.nn.relu(conv(maxpool2, conv3W, conv3b, 3, 3, 384, 1, 1, padding=\"SAME\", group=1))\n",
    "\n",
    "# Convolutional Layer 4\n",
    "conv4 = tf.nn.relu(conv(conv3, conv4W, conv4b, 3, 3, 384, 1, 1, padding=\"SAME\", group=2))\n",
    "\n",
    "# Convolutional Layer 5\n",
    "conv5 = tf.nn.relu(conv(conv4, conv5W, conv5b, 3, 3, 256, 1, 1, padding=\"SAME\", group=2))\n",
    "maxpool5 = tf.nn.max_pool(conv5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Fully-connected Layer 6\n",
    "fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(np.prod(maxpool5.get_shape()[1:]))]), fc6W,fc6b)\n",
    "\n",
    "# Fully-connected Layer 7\n",
    "fc7 = tf.nn.relu_layer(fc6, fc7W, fc7b)\n",
    "\n",
    "# Fully-connected Layer 8\n",
    "fan1 = math.sqrt(6.0 / (4096.0 + 10.0))\n",
    "fc8W = tf.Variable(tf.random_uniform([4096, 10], minval=-fan1, maxval=fan1))\n",
    "fc8b = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "fc8 = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "# Output Layer\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "cond = tf.placeholder(tf.int32, shape=[])\n",
    "y_conv = tf.nn.softmax(fc8)\n",
    "y_reshape = tf.cond(cond > 0,lambda: tf.reshape(y_conv, [NUM_BATCHES, 10, 10]),lambda: y_conv)\n",
    "y_fin = tf.cond(cond>0, lambda: tf.reduce_mean(y_reshape, reduction_indices=1), lambda: y_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross entropy setting\n",
    "temp_y = (y_ * tf.log(y_fin))\n",
    "cross_entropy = -tf.reduce_sum(temp_y)\n",
    "\n",
    "# Learning rate setting (decreases every iteration)\n",
    "lr = tf.train.exponential_decay(learning_rate=1e-4, global_step=tf.Variable(0, trainable=False), \\\n",
    "        decay_steps=num_examples//NUM_BATCHES, decay_rate=0.95, staircase=True)\n",
    "\n",
    "# Optimizer setting\n",
    "# https://www.tensorflow.org/versions/r0.11/api_docs/python/train.html#GradientDescentOptimizer\n",
    "#opt = tf.train.GradientDescentOptimizer(lr, use_locking=False, name='GradientDescent')\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.0)\n",
    "#opt = tf.train.AdagradOptimizer(lr, initial_accumulator_value=0.1, use_locking=False, name='Adagrad')\n",
    "#opt = tf.train.AdamOptimizer(lr, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
    "train_step = opt.minimize(cross_entropy, var_list=[fc8W, fc8b])\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_fin, 1), tf.argmax(y_, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "correct_2 = tf.nn.in_top_k(y_fin, tf.argmax(y_, 1), 2)\n",
    "acc_2 = tf.reduce_mean(tf.cast(correct_2, tf.float32))\n",
    "correct_3 = tf.nn.in_top_k(y_fin, tf.argmax(y_, 1), 3)\n",
    "acc_3 = tf.reduce_mean(tf.cast(correct_3, tf.float32))\n",
    "\n",
    "acc_t = tf.pack([acc, acc_2, acc_3])\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver({'fc8W': fc8W, 'fc8b': fc8b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================\n",
      "|Time\t\t\t\t|Epoch\t|Batch\t|Set\t|Top-1\t|Top-2\t|Top-3\t|\n",
      "|===============================================================================|\n",
      "|2016-12-06 10:56:47.027710\t|1\t|1\t|train\t|0.0625\t|0.2188\t|0.2500\t|\n",
      "|2016-12-06 10:57:14.374670\t|1\t|2\t|train\t|0.2188\t|0.5312\t|0.6250\t|\n",
      "|2016-12-06 10:57:41.631571\t|1\t|3\t|train\t|0.3125\t|0.4688\t|0.5625\t|\n",
      "|2016-12-06 10:58:09.135631\t|1\t|4\t|train\t|0.4062\t|0.5312\t|0.6250\t|\n",
      "|2016-12-06 10:58:36.552448\t|1\t|5\t|train\t|0.6250\t|0.7500\t|0.8125\t|\n",
      "|2016-12-06 10:59:04.044094\t|1\t|6\t|train\t|0.6562\t|0.7500\t|0.8438\t|\n",
      "|2016-12-06 10:59:31.577616\t|1\t|7\t|train\t|0.6250\t|0.6875\t|0.7500\t|\n",
      "|2016-12-06 10:59:58.739330\t|1\t|8\t|train\t|0.5312\t|0.6875\t|0.7500\t|\n",
      "|2016-12-06 11:00:25.965643\t|1\t|9\t|train\t|0.6875\t|0.7500\t|0.8125\t|\n",
      "|2016-12-06 11:00:50.046909\t|1\t|10\t|train\t|0.5938\t|0.7188\t|0.8438\t|\n",
      "|2016-12-06 11:01:17.382471\t|1\t|11\t|train\t|0.7500\t|0.8750\t|0.9062\t|\n",
      "|2016-12-06 11:01:44.528702\t|1\t|12\t|train\t|0.8438\t|0.8750\t|0.9375\t|\n",
      "|2016-12-06 11:02:11.727777\t|1\t|13\t|train\t|0.7188\t|0.8125\t|0.9062\t|\n",
      "|2016-12-06 11:02:38.839961\t|1\t|14\t|train\t|0.7188\t|0.8750\t|0.8750\t|\n",
      "|2016-12-06 11:03:06.259526\t|1\t|15\t|train\t|0.7500\t|0.8750\t|0.9375\t|\n",
      "|2016-12-06 11:03:33.200536\t|1\t|16\t|train\t|0.6875\t|0.8750\t|0.9062\t|\n",
      "|2016-12-06 11:04:01.562773\t|1\t|17\t|train\t|0.8438\t|0.8750\t|0.8750\t|\n",
      "|2016-12-06 11:04:30.548945\t|1\t|18\t|train\t|0.9062\t|0.9062\t|0.9375\t|\n",
      "|2016-12-06 11:04:58.745212\t|1\t|19\t|train\t|0.8125\t|0.8438\t|0.9062\t|\n",
      "|2016-12-06 11:05:26.660112\t|1\t|20\t|train\t|0.8750\t|0.9375\t|0.9688\t|\n",
      "|2016-12-06 11:05:54.728990\t|1\t|21\t|train\t|0.7812\t|0.8750\t|0.9688\t|\n",
      "|2016-12-06 11:06:22.984869\t|1\t|22\t|train\t|0.8750\t|0.9062\t|0.9375\t|\n",
      "|2016-12-06 11:06:50.613568\t|1\t|23\t|train\t|0.9062\t|0.9375\t|1.0000\t|\n",
      "|2016-12-06 11:07:15.212268\t|1\t|24\t|train\t|0.8438\t|0.9688\t|0.9688\t|\n",
      "|2016-12-06 11:07:38.584420\t|1\t|25\t|train\t|0.8438\t|0.9375\t|1.0000\t|\n",
      "|2016-12-06 11:07:58.519191\t|1\t|-\t|test\t|0.8889\t|0.9422\t|0.9556\t|\n",
      "|2016-12-06 11:08:11.800620\t|2\t|1\t|train\t|0.9375\t|0.9375\t|0.9375\t|\n",
      "|2016-12-06 11:08:35.237053\t|2\t|2\t|train\t|0.9688\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:08:58.850735\t|2\t|3\t|train\t|0.8125\t|0.9375\t|0.9688\t|\n",
      "|2016-12-06 11:09:22.405536\t|2\t|4\t|train\t|0.8750\t|0.9375\t|0.9375\t|\n",
      "|2016-12-06 11:09:46.312240\t|2\t|5\t|train\t|0.8438\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:10:09.617344\t|2\t|6\t|train\t|0.9062\t|0.9375\t|1.0000\t|\n",
      "|2016-12-06 11:10:36.480638\t|2\t|7\t|train\t|0.9375\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:11:04.652819\t|2\t|8\t|train\t|0.8438\t|0.9375\t|0.9688\t|\n",
      "|2016-12-06 11:11:32.438668\t|2\t|9\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:11:59.732982\t|2\t|10\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:12:28.103758\t|2\t|11\t|train\t|0.9062\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:12:56.246042\t|2\t|12\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:13:24.341075\t|2\t|13\t|train\t|0.9375\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:13:49.965433\t|2\t|14\t|train\t|0.9062\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:14:14.130673\t|2\t|15\t|train\t|0.9375\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:14:37.300410\t|2\t|16\t|train\t|0.9062\t|0.9375\t|0.9688\t|\n",
      "|2016-12-06 11:15:00.718825\t|2\t|17\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:15:23.796560\t|2\t|18\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:15:47.938597\t|2\t|19\t|train\t|0.9375\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:16:11.254254\t|2\t|20\t|train\t|0.9062\t|0.9688\t|0.9688\t|\n",
      "|2016-12-06 11:16:34.627454\t|2\t|21\t|train\t|0.9062\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:16:58.010748\t|2\t|22\t|train\t|0.9062\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:17:21.221391\t|2\t|23\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:17:44.210775\t|2\t|24\t|train\t|0.9375\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:18:07.708629\t|2\t|25\t|train\t|0.9375\t|0.9688\t|0.9688\t|\n",
      "|2016-12-06 11:18:27.162246\t|2\t|-\t|test\t|0.9333\t|0.9600\t|0.9778\t|\n",
      "|2016-12-06 11:18:39.992249\t|3\t|1\t|train\t|0.9062\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:19:02.942966\t|3\t|2\t|train\t|0.9062\t|0.9688\t|0.9688\t|\n",
      "|2016-12-06 11:19:25.963765\t|3\t|3\t|train\t|0.8750\t|0.9688\t|0.9688\t|\n",
      "|2016-12-06 11:19:49.415206\t|3\t|4\t|train\t|0.9062\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:20:12.886178\t|3\t|5\t|train\t|0.9375\t|0.9375\t|1.0000\t|\n",
      "|2016-12-06 11:20:36.199854\t|3\t|6\t|train\t|0.9688\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:20:59.382670\t|3\t|7\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:21:22.584394\t|3\t|8\t|train\t|0.9375\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:21:46.149652\t|3\t|9\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:22:09.032826\t|3\t|10\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:22:32.449336\t|3\t|11\t|train\t|0.9375\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:22:55.475683\t|3\t|12\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:23:19.117496\t|3\t|13\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:23:42.309489\t|3\t|14\t|train\t|0.9688\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:24:05.461321\t|3\t|15\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:24:29.157281\t|3\t|16\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:24:52.355645\t|3\t|17\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:25:15.512889\t|3\t|18\t|train\t|0.9062\t|0.9688\t|0.9688\t|\n",
      "|2016-12-06 11:25:38.616847\t|3\t|19\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:26:01.841379\t|3\t|20\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:26:24.836884\t|3\t|21\t|train\t|1.0000\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:26:47.755800\t|3\t|22\t|train\t|0.9375\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:27:11.092817\t|3\t|23\t|train\t|0.9688\t|0.9688\t|1.0000\t|\n",
      "|2016-12-06 11:27:34.251219\t|3\t|24\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:27:56.976647\t|3\t|25\t|train\t|0.9688\t|1.0000\t|1.0000\t|\n",
      "|2016-12-06 11:28:17.249354\t|3\t|-\t|test\t|0.9422\t|0.9689\t|0.9778\t|\n",
      "Fine-tuned network saved in file: ./finetune_fc.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Internal variables for next_batch function\n",
    "epochs_completed = 0\n",
    "num_examples = X.shape[0]\n",
    "index_in_epoch = num_examples + 1\n",
    "\n",
    "# next_batch\n",
    "def next_batch(batch_size):\n",
    "    global X, Y\n",
    "    global index_in_epoch, epochs_completed\n",
    "\n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "\n",
    "    # when all trainig data has been used, it is reordered randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        X, Y = X[perm], Y[perm]\n",
    "\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "\n",
    "    X_crop = crop_batch(X[start:end], mean_data)\n",
    "    return X_crop, Y[start:end]\n",
    "\n",
    "\n",
    "print(\n",
    "    \"=================================================================================\"\n",
    ")\n",
    "print(\"|Time\\t\\t\\t\\t|Epoch\\t|Batch\\t|Set\\t|Top-1\\t|Top-2\\t|Top-3\\t|\")\n",
    "print(\n",
    "    \"|===============================================================================|\"\n",
    ")\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(NUM_EPOCH):\n",
    "    for j in range(num_examples // NUM_BATCHES):\n",
    "        Xb, Yb = next_batch(NUM_BATCHES)\n",
    "        #acc_print=y_.eval(feed_dict={x:Xb,y_:Yb,cond:1},session=sess)\n",
    "        train_acc = acc_t.eval(\n",
    "            feed_dict={x: Xb,\n",
    "                       y_: Yb,\n",
    "                       cond: 1}, session=sess)\n",
    "        print(\"|\" + str(datetime.now()) +\n",
    "              \"\\t|%d\\t|%d\\t|train\\t|%4.4f\\t|%4.4f\\t|%4.4f\\t|\" % (\n",
    "                  i + 1, j + 1, train_acc[0], train_acc[1], train_acc[2]))\n",
    "        sess.run(train_step, feed_dict={x: Xb, y_: Yb, cond: 1})\n",
    "    test_acc = acc_t.eval(\n",
    "        feed_dict={x: Xb_val,\n",
    "                   y_: Y_val,\n",
    "                   cond: -1}, session=sess)\n",
    "    print(\"|\" + str(datetime.now()) +\n",
    "          \"\\t|%d\\t|-\\t|test\\t|%4.4f\\t|%4.4f\\t|%4.4f\\t|\" % (i + 1, test_acc[\n",
    "              0], test_acc[1], test_acc[2]))\n",
    "    npy_save = {}\n",
    "    npy_save[0] = fc8W.eval()\n",
    "    npy_save[1] = fc8b.eval()\n",
    "    np.save(output_weight_filename, npy_save)\n",
    "\n",
    "save_path = saver.save(sess, \"./finetune_fc.ckpt\")\n",
    "print(\"Fine-tuned network saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
